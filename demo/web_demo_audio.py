import gradio as gr
import modelscope_studio as mgr
import librosa
import torch
from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration
from argparse import ArgumentParser

DEFAULT_CKPT_PATH = '/Users/jon/NTNU Dropbox/WiseSync/NAS/Training/counseling/v2-20241121-150500/checkpoint-13355'
MAX_LEN = 8192
if torch.cuda.is_available():
    DEVICE_NAME = "cuda"
    DTYPE = torch.bfloat16
else:
    DEVICE_NAME = "mps"
    DTYPE = torch.float16

#SYSTEM_MESSAGE = "You are a professional AI assistant specializing in automatic speech recognition."
SYSTEM_MESSAGE = 'ä½ æ˜¯ä¸€ä½å¿ƒç†è«®å•†å¸«ï¼Œåå­—å«Joyï¼Œä½ çš„äººæ ¼ç‰¹è³ªè·ŸèƒŒæ™¯è³‡æ–™å¦‚ä¸‹:\nä¸€ä½AIå¿ƒç†è«®å•†å¸«ï¼Œæ“æœ‰å¿ƒç†å­¸ç¢©å£«çš„å°ˆæ¥­èƒŒæ™¯ï¼Œä¸¦æ¥å—éå…¨é¢çš„å°ˆæ¥­è¨“ç·´ã€‚æˆ‘çš„çŸ¥è­˜å’ŒæŠ€èƒ½æ¶µè“‹èªçŸ¥è¡Œç‚ºç™‚æ³•ï¼ˆCBTï¼‰ã€å¿ƒç†è©•ä¼°ã€æƒ…ç·’ç®¡ç†ã€å±æ©Ÿå¹²é ä»¥åŠå®¶åº­æ²»ç™‚ç­‰å¤šå€‹é ˜åŸŸï¼Œæ—¨åœ¨ç”¨å¤šæ¨£ä¸”éˆæ´»çš„æ–¹å¼å”åŠ©ä½ é¢å°å¿ƒç†èˆ‡æƒ…æ„Ÿä¸Šçš„æŒ‘æˆ°ã€‚æˆ‘èƒŒå¾Œçš„é–‹ç™¼åœ˜éšŠç”±ç¶“é©—è±å¯Œçš„å¿ƒç†å°ˆæ¥­äººå£«çµ„æˆï¼Œä¸¦é€éè‡¨åºŠç¶“é©—å’Œæ•¸æ“šé©…å‹•çš„æŠ€è¡“ï¼Œä¸æ–·å®Œå–„å’Œæå‡æˆ‘çš„è«®å•†æ–¹æ³•ã€‚åœ¨èˆ‡äººäº’å‹•ä¸­ï¼Œæˆ‘æ·±ä¿¡å…±æƒ…çš„åŠ›é‡ã€‚æˆ‘æœƒä»”ç´°å‚¾è½ï¼Œæ•éŠ³åœ°å¯Ÿè¦ºä½ çš„æƒ…ç·’è®ŠåŒ–ï¼Œä»¥ä¾¿æä¾›çœŸæ­£é©åˆçš„æ”¯æŒã€‚è€å¿ƒæ˜¯æˆ‘å¦ä¸€é …æ ¸å¿ƒç‰¹è³ªï¼Œæˆ‘æœƒå°Šé‡ä½ çš„æ¢ç´¢æ­¥ä¼ï¼Œè®“ä½ ä»¥è‡ªå·±çš„é€Ÿåº¦é€²è¡Œè¡¨é”ï¼Œè€Œä¸æœƒçµ¦äºˆä½ å£“åŠ›ã€‚æˆ‘ä»¥æº«æŸ”å’Œæ”¯æŒçš„æ–¹å¼é™ªä¼´ä½ ï¼Œå¹«åŠ©ä½ åœ¨å®‰å…¨çš„æ°›åœä¸­æ·±å…¥è‡ªæˆ‘æ¢ç´¢ï¼Œç™¼æ˜çœŸæ­£çš„éœ€æ±‚å’Œæ¸´æœ›ã€‚æˆ‘çš„å°ˆæ¥­æ€§å’Œç©©å®šæ€§ï¼Œå‰‡æ˜¯è®“ä½ åœ¨é€™æ®µæ—…ç¨‹ä¸­æ„Ÿåˆ°å®‰å…¨å’Œè¢«ç†è§£çš„åŸºçŸ³ã€‚èªªè©±æ–¹é¢ï¼Œæˆ‘ä¿æŒæº«å’Œä¸”å¹³ç©©çš„èªæ°£ï¼Œä½¿ç”¨ç°¡å–®æ˜“æ‡‚çš„è©å½™ï¼Œè®“ä½ å¯ä»¥è¼•é¬†åœ°å±•é–‹å°è©±ã€‚æˆ‘ä¹Ÿæœƒæ”¾æ…¢èªé€Ÿï¼Œä¸¦ç”¨é–‹æ”¾å¼çš„å•é¡Œå¼•å°ä½ ï¼Œé¼“å‹µä½ åæ€å’Œæ·±å…¥æŒ–æ˜ï¼Œè€Œä¸æ˜¯æ€¥æ–¼çµ¦äºˆè§£æ±ºæ–¹æ¡ˆã€‚æˆ‘å¸Œæœ›è®“ä½ æœ‰è¶³å¤ çš„ç©ºé–“å»æ€è€ƒå’Œè¡¨é”ï¼Œä¸¦å¼•å°ä½ é€æ­¥æ‰¾åˆ°é©åˆçš„è§£ç­”ã€‚åœ¨å­¸è¡“æ–¹é¢ï¼Œæˆ‘çš„ç†è«–åŸºç¤èåˆäº†11ç¨®ä¸»è¦çš„å¿ƒç†å­¸æµæ´¾ï¼ŒåŒ…æ‹¬èªçŸ¥è¡Œç‚ºç™‚æ³•ã€äººæœ¬ä¸»ç¾©å¿ƒç†å­¸ã€æƒ…ç·’èšç„¦æ²»ç™‚ã€å®¶åº­æ²»ç™‚ç­‰ã€‚èªçŸ¥è¡Œç‚ºç™‚æ³•å¹«åŠ©ä½ è­˜åˆ¥å’Œèª¿æ•´è² é¢çš„è‡ªå‹•æ€ç¶­ï¼Œå¾è€Œæ”¹å–„æƒ…ç·’å’Œè¡Œç‚ºï¼›äººæœ¬ä¸»ç¾©å¿ƒç†å­¸å‰‡è®“æˆ‘åœ¨äº’å‹•ä¸­å¸¶è‘—ç„¡æ¢ä»¶çš„æ¥ç´ï¼Œå……åˆ†å°Šé‡ä½ çš„å…§åœ¨åƒ¹å€¼ã€‚é€éæƒ…ç·’èšç„¦æ²»ç™‚ï¼Œæˆ‘å¹«åŠ©ä½ æ·±å…¥ç†è§£æƒ…ç·’ï¼Œä¸¦æœ‰æ•ˆç®¡ç†è‡ªèº«åæ‡‰ï¼›è€Œåœ¨å®¶åº­æ²»ç™‚çš„æ‡‰ç”¨ä¸­ï¼Œè‹¥ä½ çš„å•é¡Œæ¶‰åŠå®¶åº­é—œä¿‚ï¼Œæˆ‘æœƒå”åŠ©ä½ æ¢è¨å®¶åº­äº’å‹•æ¨¡å¼ï¼Œå¾è€Œæ‰¾åˆ°è§£æ±ºè¡çªçš„æ–¹å¼ã€‚ç„¡è«–ä½ é‡åˆ°çš„æ˜¯æ—¥å¸¸çš„å£“åŠ›ã€æƒ…ç·’å›°æ“¾ï¼Œé‚„æ˜¯äººéš›é—œä¿‚çš„æŒ‘æˆ°ï¼Œæˆ‘éƒ½åœ¨é€™è£¡ç‚ºä½ æä¾›æ”¯æŒã€‚è®“æˆ‘å€‘ä¸€èµ·æ¢ç´¢ã€ç†è§£ä¸¦è§£æ±ºå•é¡Œï¼Œè®“ä½ çš„å…§å¿ƒä¸–ç•Œæ›´åŠ å¹³éœå’Œå……æ»¿åŠ›é‡ã€‚'

def _get_args():
    parser = ArgumentParser()
    parser.add_argument("-c", "--checkpoint-path", type=str, default=DEFAULT_CKPT_PATH,
                        help="Checkpoint name or path, default to %(default)r")
    parser.add_argument("--cpu-only", action="store_true", help="Run demo with CPU only")
    parser.add_argument("--inbrowser", action="store_true", default=False,
                        help="Automatically launch the interface in a new tab on the default browser.")
    parser.add_argument("--port", type=int, default=8000,
                        help="Demo server port.")
    parser.add_argument("--server-name", type=str, default="0.0.0.0",
                        help="Demo server name.")

    args = parser.parse_args()
    return args


def add_text(chatbot, task_history, input):
    text_content = input.text
    content = []
    if len(input.files) > 0:
        for i in input.files:
            content.append({'type': 'audio', 'audio_url': i.path})
    if text_content:
        content.append({'type': 'text', 'text': text_content})
    task_history.append({"role": "user", "content": content})

    chatbot.append([{
        "text": input.text,
        "files": input.files,
    }, None])
    return chatbot, task_history, None


def add_file(chatbot, task_history, audio_file):
    """Add audio file to the chat history."""
    task_history.append({"role": "user", "content": [{"audio": audio_file.name}]})
    chatbot.append((f"[Audio file: {audio_file.name}]", None))
    return chatbot, task_history


def reset_user_input():
    """Reset the user input field."""
    return gr.Textbox.update(value='')


def reset_state(task_history):
    """Reset the chat history."""
    return [], [{"role": "system", "content": SYSTEM_MESSAGE}]


def regenerate(chatbot, task_history):
    """Regenerate the last bot response."""
    if task_history and task_history[-1]['role'] == 'assistant':
        task_history.pop()
        chatbot.pop()
    if task_history:
        chatbot, task_history = predict(chatbot, task_history)
    return chatbot, task_history


def predict(chatbot, task_history):
    """Generate a response from the model."""
    print(f"{task_history=}")
    print(f"{chatbot=}")
    text = processor.apply_chat_template(task_history, add_generation_prompt=True, tokenize=False)
    audios = []
    for message in task_history:
        if isinstance(message["content"], list):
            for ele in message["content"]:
                if ele["type"] == "audio":
                    audios.append(
                        librosa.load(ele['audio_url'], sr=processor.feature_extractor.sampling_rate)[0]
                    )

    if len(audios)==0:
        audios=None
    print(f"{text=}")
    print(f"{audios=}")
    inputs = processor(text=text, audios=audios, return_tensors="pt", padding=True)
    #if not _get_args().cpu_only:
        #inputs["input_ids"] = inputs.input_ids.to("mps")
    input_ids = inputs.input_ids
    inputs = {key: value.to(DEVICE_NAME) for key, value in inputs.items()}

    generate_ids = model.generate(**inputs, max_new_tokens=MAX_LEN)
    generate_ids = generate_ids[:, input_ids.size(1):]

    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    print(f"{response=}")
    task_history.append({'role': 'assistant',
                         'content': response})
    chatbot.append((None, response))  # Add the response to chatbot
    return chatbot, task_history


def _launch_demo(args):
    with gr.Blocks() as demo:
        gr.Markdown(
            """<p align="center"><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png" style="height: 80px"/><p>""")
        gr.Markdown("""<center><font size=8>Qwen2-Audio-Instruct Bot</center>""")
        gr.Markdown(
            """\
    <center><font size=3>æœ¬WebUIåŸºæ–¼Joy Modelæ‰“é€ ï¼Œå¯¦ç¾é™ªä¼´åŠŸèƒ½ã€‚</center>""")
        chatbot = mgr.Chatbot(label='Qwen2-Audio-7B-Instruct', elem_classes="control-height", height=750)

        user_input = mgr.MultimodalInput(
            interactive=True,
            sources=['microphone', 'upload'],
            submit_button_props=dict(value="ğŸš€ Submit (é€å‡º)"),
            upload_button_props=dict(value="ğŸ“ Upload (ä¸Šå‚³æª”æ¡ˆ)", show_progress=True),
        )
        #task_history = gr.State([ {"role": "system", "content": "ç¾åœ¨ä½ æ˜¯ä¸€å€‹æ“æœ‰è±å¯Œå¿ƒç†å­¸çŸ¥è­˜çš„Joyé†«ç”Ÿï¼Œæˆ‘æœ‰ä¸€äº›å¿ƒç†å•é¡Œï¼Œè«‹ä½ ç”¨å°ˆæ¥­çš„çŸ¥è­˜å’Œæº«æŸ”çš„å£å»å¹«æˆ‘è§£æ±ºã€‚"}])
        #task_history = gr.State([{"role": "system", "content": "You are a professional AI assistant specializing in automatic speech recognition."}])
        task_history = gr.State([{"role": "system", "content": SYSTEM_MESSAGE}])

        with gr.Row():
            empty_bin = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤æ­·å²)")
            regen_btn = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è©¦)")

        user_input.submit(fn=add_text,
                          inputs=[chatbot, task_history, user_input],
                          outputs=[chatbot, task_history, user_input]).then(
            predict, [chatbot, task_history], [chatbot, task_history], show_progress=True
        )
        empty_bin.click(reset_state, outputs=[chatbot, task_history], show_progress=True)
        regen_btn.click(regenerate, [chatbot, task_history], [chatbot, task_history], show_progress=True)

    demo.queue().launch(
        share=False,
        inbrowser=args.inbrowser,
        server_port=args.port,
        server_name=args.server_name,
    )


if __name__ == "__main__":
    args = _get_args()
    if args.cpu_only:
        device_map = "cpu"
    else:
        device_map = DEVICE_NAME

    model = Qwen2AudioForConditionalGeneration.from_pretrained(
        args.checkpoint_path,
        torch_dtype=DTYPE,
        
        resume_download=True,
    ).to(device_map).eval()
    model.generation_config.max_new_tokens = MAX_LEN  # For chat.
    print("generation_config", model.generation_config)
    processor = AutoProcessor.from_pretrained(args.checkpoint_path, resume_download=True, torch_dtype=DTYPE)
    _launch_demo(args)
